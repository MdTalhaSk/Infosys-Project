{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNx0hqOUh4OKJ0nG9w2oV0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11priyanshu/INFOSYS-SPRINGBOARD-PROJECT/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PYSPARK**"
      ],
      "metadata": {
        "id": "NcyjpbAke32J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is the Python API for Apache Spark, a powerful distributed computing framework designed for big data processing. It enables Python developers to leverage Spark's capabilities for large-scale data processing, machine learning, and analytics.\n",
        "\n",
        "Key Features of PySpark:\n",
        "Distributed Computing: Processes large datasets across a cluster of machines.\n",
        "Ease of Use: Python interface simplifies coding with Spark.\n",
        "Scalability: Handles datasets too large for a single machine.\n",
        "Integration: Works seamlessly with tools like Hadoop, Hive, and SQL databases.\n",
        "Key Components in PySpark:\n",
        "RDD (Resilient Distributed Dataset):\n",
        "\n",
        "Low-level data structure for fault-tolerant distributed computation.\n",
        "Operations: map, filter, reduce.\n",
        "DataFrame:\n",
        "\n",
        "High-level abstraction for structured data, similar to pandas.\n",
        "Supports SQL queries and is optimized for performance.\n",
        "Spark SQL:\n",
        "\n",
        "Executes SQL queries on DataFrames.\n",
        "Integrates seamlessly with other components.\n",
        "MLlib:\n",
        "\n",
        "Machine learning library in PySpark.\n",
        "Provides algorithms for classification, regression, clustering, etc.\n",
        "Spark Streaming:\n",
        "\n",
        "Real-time data processing."
      ],
      "metadata": {
        "id": "HXGzlnXmfSRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating a PySpark Session"
      ],
      "metadata": {
        "id": "hajP4laIffb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Example\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "YymmnfjRfiv3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Creating a DataFrame"
      ],
      "metadata": {
        "id": "ipv241erfqGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Carol\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "f1Rr59AmfxCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using SQL Queries on DataFrames"
      ],
      "metadata": {
        "id": "tq6pYPUOfzcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "query = \"SELECT Name, Age FROM people WHERE Age > 30\"\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ],
      "metadata": {
        "id": "oL3ZR-Suf3un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Performing Transformations"
      ],
      "metadata": {
        "id": "FAn6eOgFf6ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(df.Age > 30)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "id": "naCAN-gwf-9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Writing Data to Storage"
      ],
      "metadata": {
        "id": "8JtB_c77gDxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.csv(\"output/people.csv\")"
      ],
      "metadata": {
        "id": "Kw8scNicgEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use PySpark:\n",
        "\n",
        "1. Processing datasets larger than your system's memory.\n",
        "\n",
        "2. Distributed data processing and ETL pipelines.\n",
        "\n",
        "3. Real-time analytics on streaming data.\n",
        "\n",
        "4. Machine learning at scale."
      ],
      "metadata": {
        "id": "57dmVgVwgHe4"
      }
    }
  ]
}