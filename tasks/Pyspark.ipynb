{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7rhuwEYgW0JYhTSkzg/JJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdTalhaSk/Infosys-Project/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is PySpark?**\n",
        "\n",
        "PySpark is the Python API for Apache Spark, an open-source distributed computing framework designed for big data processing and analytics.\n",
        "It allows you to leverage the power of Spark using Python, making it easier to perform distributed data processing on large datasets.\n",
        "\n",
        "**Key Features of PySpark:**\n",
        "\n",
        "**Fast Data Processing:** Built for handling big data efficiently using in-memory computations.\n",
        "\n",
        "**Distributed Computing:** Distributes tasks across a cluster of machines for parallel processing.\n",
        "\n",
        "**Support for Multiple Data Sources:** Works with structured and unstructured data, including CSV, JSON, Parquet, Hive, HDFS, etc.\n",
        "\n",
        "**APIs:**\n",
        "\n",
        "Spark SQL for structured data.\n",
        "\n",
        "DataFrame API for data manipulation.\n",
        "\n",
        "Spark MLlib for machine learning.\n",
        "\n",
        "GraphX for graph processing.\n",
        "\n",
        "**Scalability:** Scales from a single machine to thousands of machines.\n",
        "\n",
        "**Why Use PySpark?**\n",
        "\n",
        "**Big Data:** Designed to process huge datasets efficiently.\n",
        "\n",
        "**Ease of Use:** Combines Spark's performance with Python's simplicity.\n",
        "\n",
        "**Integrations:** Works seamlessly with Hadoop, HDFS, and other big data tools.\n",
        "\n",
        "**Machine Learning:** Provides MLlib, a distributed machine learning library.\n",
        "\n",
        "**Core Components of PySpark:**\n",
        "\n",
        "**RDD (Resilient Distributed Dataset):**\n",
        "\n",
        "Low-level, immutable distributed collection of data.\n",
        "Provides fault tolerance and distributed processing.\n",
        "\n",
        "**DataFrame:**\n",
        "Higher-level abstraction of RDDs.\n",
        "Similar to Pandas DataFrame but distributed across a cluster.\n",
        "Optimized for query execution using Catalyst Optimizer.\n",
        "\n",
        "**Spark SQL:**\n",
        "Provides SQL-like interface for querying structured data.\n",
        "\n",
        "**Spark MLlib:**\n",
        "A scalable library for machine learning algorithms.\n",
        "\n",
        "**Spark Streaming:**\n",
        "Processes real-time data streams.\n"
      ],
      "metadata": {
        "id": "A8oaphRevIKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up PySpark:**\n",
        "\n",
        "To use PySpark, you need to install it and configure the environment.\n",
        "\n",
        "**1. Installation:**"
      ],
      "metadata": {
        "id": "XuiGGGSPv8LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "id": "v7wLgSAdv-IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Running PySpark:**\n",
        "\n",
        "Start the PySpark shell:"
      ],
      "metadata": {
        "id": "mtZpdL0Yv_R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark\n"
      ],
      "metadata": {
        "id": "eSJBVTgAwEl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
